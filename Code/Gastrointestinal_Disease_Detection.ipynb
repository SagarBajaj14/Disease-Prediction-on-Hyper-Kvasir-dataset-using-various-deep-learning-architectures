{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dp913/CSE-541-Computer-Vision-2023-Group-8/blob/master/Code/Gastrointestinal_Disease_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2a18wwVmMMA"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Conv2D,Flatten,Dense,MaxPool2D,BatchNormalization,GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input,decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYLcliJ3mXHP",
        "outputId": "764bef5c-5787-4d56-a252-8fb00dcc0648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Classification"
      ],
      "metadata": {
        "id": "rD7QCQbu1vP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image size and batch size\n",
        "img_height, img_width = (224,224)\n",
        "batch_size = 32\n",
        "\n",
        "# Load the train, test and validation sets\n",
        "train_data_dir = \"/content/drive/MyDrive/split_dataset/train\"\n",
        "val_data_dir = \"/content/drive/MyDrive/split_dataset/val\"\n",
        "test_data_dir = \"/content/drive/MyDrive/split_dataset/test\"\n"
      ],
      "metadata": {
        "id": "K4IRcyyDmZ3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Augmentation using the ImageDataGenerator \n",
        "\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   shear_range=0.3,       # specifies the range for randomly applying shear transformations to the input images during data augmentation\n",
        "                                   zoom_range=0.2,        # specifies the range for randomly applying zoom transformations to the input images during data augmentation.\n",
        "                                   width_shift_range=0.1,\n",
        "                                   height_shift_range=0.1,\n",
        "                                   horizontal_flip=True,  # specifies whether to randomly flip the images horizontally during data augmentation\n",
        "                                   validation_split=0.4)  # specifies the proportion of the training data that should be reserved for validation during model training.\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),  # The dimensions to which all images found will be resized.\n",
        "    batch_size = batch_size,              # Size of the batches of data (default: 32).\n",
        "    class_mode = 'categorical',           # Determines the type of label arrays that are returned: \"categorical\" will be 2D one-hot encoded labels\n",
        "    subset = 'training')                  # Subset of data (\"training\" or \"validation\") if validation_split is set in ImageDataGenerator.\n",
        "\n",
        "valid_generator = train_datagen.flow_from_directory(\n",
        "    val_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'categorical',\n",
        "    subset = 'validation')\n",
        "\n",
        "test_generator = train_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    class_mode = 'categorical',\n",
        "    subset = 'validation')\n"
      ],
      "metadata": {
        "id": "-JRPHYRsmhhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6368b555-6d49-4b29-aaca-a6aa8f8e2a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 949 images belonging to 4 classes.\n",
            "Found 203 images belonging to 4 classes.\n",
            "Found 211 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained ResNet50 model from keras\n",
        "base_model = ResNet50(include_top=False, weights='imagenet')\n",
        "x = base_model.output\n",
        "\n",
        "# Apply Global Average Pooling on the output layer\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add a layer with 1024 neurons and relu activation\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# Define the output layer with 4 classes and Softmax activation\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Add the output layer to the model\n",
        "resnet_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Compile the Model with appropriate optimizer and loss function\n",
        "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Train the model on train set\n",
        "resnet_model.fit(train_generator, epochs=30, validation_data = valid_generator)"
      ],
      "metadata": {
        "id": "53QTFw3wml0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5bb84d5-2493-4ce2-fc6c-b2b457589703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n",
            "Epoch 1/30\n",
            "30/30 [==============================] - 426s 14s/step - loss: 1.6097 - accuracy: 0.7713 - val_loss: 0.5724 - val_accuracy: 0.8867\n",
            "Epoch 2/30\n",
            "30/30 [==============================] - 31s 1s/step - loss: 0.3246 - accuracy: 0.9094 - val_loss: 0.2229 - val_accuracy: 0.9163\n",
            "Epoch 3/30\n",
            "30/30 [==============================] - 30s 1s/step - loss: 0.1846 - accuracy: 0.9283 - val_loss: 0.3817 - val_accuracy: 0.8719\n",
            "Epoch 4/30\n",
            "30/30 [==============================] - 30s 1s/step - loss: 0.1509 - accuracy: 0.9473 - val_loss: 0.2712 - val_accuracy: 0.9212\n",
            "Epoch 5/30\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.1247 - accuracy: 0.9568 - val_loss: 0.2708 - val_accuracy: 0.9310\n",
            "Epoch 6/30\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.1589 - accuracy: 0.9484 - val_loss: 0.2434 - val_accuracy: 0.9163\n",
            "Epoch 7/30\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.1247 - accuracy: 0.9526 - val_loss: 0.1917 - val_accuracy: 0.9360\n",
            "Epoch 8/30\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.1034 - accuracy: 0.9621 - val_loss: 0.2316 - val_accuracy: 0.9163\n",
            "Epoch 9/30\n",
            "30/30 [==============================] - 30s 1s/step - loss: 0.1048 - accuracy: 0.9568 - val_loss: 0.2389 - val_accuracy: 0.9310\n",
            "Epoch 10/30\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.1121 - accuracy: 0.9526 - val_loss: 0.2474 - val_accuracy: 0.9261\n",
            "Epoch 11/30\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.1084 - accuracy: 0.9673 - val_loss: 0.1956 - val_accuracy: 0.9507\n",
            "Epoch 12/30\n",
            "30/30 [==============================] - 30s 1s/step - loss: 0.0830 - accuracy: 0.9621 - val_loss: 0.2676 - val_accuracy: 0.9261\n",
            "Epoch 13/30\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0849 - accuracy: 0.9705 - val_loss: 0.2466 - val_accuracy: 0.9212\n",
            "Epoch 14/30\n",
            "30/30 [==============================] - 30s 1s/step - loss: 0.1165 - accuracy: 0.9515 - val_loss: 0.2478 - val_accuracy: 0.9113\n",
            "Epoch 15/30\n",
            "30/30 [==============================] - 30s 1s/step - loss: 0.0798 - accuracy: 0.9737 - val_loss: 0.2359 - val_accuracy: 0.9064\n",
            "Epoch 16/30\n",
            "30/30 [==============================] - 31s 1s/step - loss: 0.0757 - accuracy: 0.9715 - val_loss: 0.2890 - val_accuracy: 0.9064\n",
            "Epoch 17/30\n",
            "30/30 [==============================] - 30s 998ms/step - loss: 0.1105 - accuracy: 0.9579 - val_loss: 0.2895 - val_accuracy: 0.9163\n",
            "Epoch 18/30\n",
            "30/30 [==============================] - 31s 1s/step - loss: 0.0636 - accuracy: 0.9747 - val_loss: 0.2141 - val_accuracy: 0.9261\n",
            "Epoch 19/30\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.0750 - accuracy: 0.9747 - val_loss: 0.2776 - val_accuracy: 0.9310\n",
            "Epoch 20/30\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.0702 - accuracy: 0.9789 - val_loss: 0.2707 - val_accuracy: 0.9212\n",
            "Epoch 21/30\n",
            "30/30 [==============================] - 33s 1s/step - loss: 0.0876 - accuracy: 0.9673 - val_loss: 0.3037 - val_accuracy: 0.9015\n",
            "Epoch 22/30\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.0629 - accuracy: 0.9747 - val_loss: 0.2889 - val_accuracy: 0.9360\n",
            "Epoch 23/30\n",
            "30/30 [==============================] - 31s 1s/step - loss: 0.0646 - accuracy: 0.9758 - val_loss: 0.3412 - val_accuracy: 0.9360\n",
            "Epoch 24/30\n",
            "30/30 [==============================] - 33s 1s/step - loss: 0.0622 - accuracy: 0.9779 - val_loss: 0.2957 - val_accuracy: 0.9310\n",
            "Epoch 25/30\n",
            "30/30 [==============================] - 33s 1s/step - loss: 0.0814 - accuracy: 0.9694 - val_loss: 0.2596 - val_accuracy: 0.9212\n",
            "Epoch 26/30\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.0796 - accuracy: 0.9694 - val_loss: 0.3223 - val_accuracy: 0.9113\n",
            "Epoch 27/30\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0731 - accuracy: 0.9694 - val_loss: 0.3452 - val_accuracy: 0.9310\n",
            "Epoch 28/30\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.0561 - accuracy: 0.9779 - val_loss: 0.2560 - val_accuracy: 0.9310\n",
            "Epoch 29/30\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.0368 - accuracy: 0.9863 - val_loss: 0.3715 - val_accuracy: 0.8621\n",
            "Epoch 30/30\n",
            "30/30 [==============================] - 34s 1s/step - loss: 0.0437 - accuracy: 0.9810 - val_loss: 0.2590 - val_accuracy: 0.9212\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcccc5336a0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test set\n",
        "\n",
        "test_loss, test_acc = resnet_model.evaluate(test_generator)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "l8dW6yJdmpnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a1de692-fc3e-4d9b-e34a-063a7a9b95f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "211/211 [==============================] - 117s 553ms/step - loss: 0.2759 - accuracy: 0.9242\n",
            "Test loss: 0.27587711811065674\n",
            "Test accuracy: 0.9241706132888794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "resnet_model.save('/content/drive/My Drive/models/resnet50_images.h5')"
      ],
      "metadata": {
        "id": "-sBWOHoGJtrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Video Classification"
      ],
      "metadata": {
        "id": "qWT8oUhe1ozC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Video dataset path\n",
        "dataset_path = '/content/drive/MyDrive/Video_Dataset3'"
      ],
      "metadata": {
        "id": "gBC8Rdpnm9rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model = load_model('/content/drive/MyDrive/models/resnet50_images.h5')\n",
        "\n",
        "flatten_layer = tensorflow.keras.layers.Flatten()(model.output)\n",
        "output_layer = tensorflow.keras.layers.Dense(1, activation='relu')(flatten_layer)\n",
        "\n",
        "# Create the full model by combining the ResNet50 model with the classification layers\n",
        "full_model = tensorflow.keras.models.Model(inputs=model.input, outputs=output_layer)"
      ],
      "metadata": {
        "id": "Leb2lS1GKMer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to extract frames from videos and preprocess them for use with the ResNet50 model\n",
        "def preprocess_video(video_path):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, (224, 224))\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    print(\"Frames = \", len(frames))\n",
        "    frames = np.array(frames) / 255.0\n",
        "    return frames"
      ],
      "metadata": {
        "id": "PNvuqJyHmwZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the videos in the dataset and extract their features using the ResNet50 model\n",
        "features = []\n",
        "labels = []\n",
        "for label in os.listdir(dataset_path):\n",
        "    label_path = os.path.join(dataset_path, label)\n",
        "    print(\"Label: \",label)\n",
        "    for video_file in os.listdir(label_path):\n",
        "        video_path = os.path.join(label_path, video_file)\n",
        "        frames = preprocess_video(video_path)\n",
        "        video_features = full_model.predict(frames)\n",
        "        video_features = np.max(video_features, axis=0)\n",
        "        features.append(video_features)\n",
        "        labels.append(label)"
      ],
      "metadata": {
        "id": "7S4OO0y2nwji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad07b23-b5d8-4cbd-bf98-1559b9fcfcda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label:  Polyps\n",
            "Frames =  199\n",
            "7/7 [==============================] - 1s 98ms/step\n",
            "Frames =  567\n",
            "18/18 [==============================] - 2s 96ms/step\n",
            "Frames =  143\n",
            "5/5 [==============================] - 0s 90ms/step\n",
            "Frames =  449\n",
            "15/15 [==============================] - 1s 98ms/step\n",
            "Frames =  752\n",
            "24/24 [==============================] - 2s 98ms/step\n",
            "Frames =  387\n",
            "13/13 [==============================] - 1s 97ms/step\n",
            "Frames =  295\n",
            "10/10 [==============================] - 1s 91ms/step\n",
            "Frames =  601\n",
            "19/19 [==============================] - 2s 93ms/step\n",
            "Frames =  677\n",
            "22/22 [==============================] - 2s 91ms/step\n",
            "Frames =  370\n",
            "12/12 [==============================] - 1s 91ms/step\n",
            "Label:  healthy\n",
            "Frames =  300\n",
            "10/10 [==============================] - 1s 91ms/step\n",
            "Frames =  300\n",
            "10/10 [==============================] - 1s 86ms/step\n",
            "Frames =  300\n",
            "10/10 [==============================] - 1s 92ms/step\n",
            "Frames =  300\n",
            "10/10 [==============================] - 1s 91ms/step\n",
            "Frames =  300\n",
            "10/10 [==============================] - 1s 92ms/step\n",
            "Frames =  250\n",
            "8/8 [==============================] - 1s 92ms/step\n",
            "Frames =  250\n",
            "8/8 [==============================] - 1s 86ms/step\n",
            "Frames =  250\n",
            "8/8 [==============================] - 1s 86ms/step\n",
            "Frames =  250\n",
            "8/8 [==============================] - 1s 95ms/step\n",
            "Frames =  250\n",
            "8/8 [==============================] - 1s 94ms/step\n",
            "Frames =  240\n",
            "8/8 [==============================] - 1s 87ms/step\n",
            "Frames =  240\n",
            "8/8 [==============================] - 1s 88ms/step\n",
            "Frames =  240\n",
            "8/8 [==============================] - 1s 88ms/step\n",
            "Frames =  240\n",
            "8/8 [==============================] - 1s 88ms/step\n",
            "Frames =  240\n",
            "8/8 [==============================] - 1s 89ms/step\n",
            "Frames =  240\n",
            "8/8 [==============================] - 1s 95ms/step\n",
            "Frames =  240\n",
            "8/8 [==============================] - 1s 93ms/step\n",
            "Frames =  240\n",
            "8/8 [==============================] - 1s 93ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the features and labels to numpy arrays and one-hot encode the labels\n",
        "\n",
        "features = np.array(features)\n",
        "# print(\"labels:\",labels)\n",
        "new_labels = labels\n",
        "#labels = np.array(labels)\n",
        "new_labels = np.array(new_labels)\n",
        "for i in range(len(new_labels)):\n",
        "  if new_labels[i] == 'healthy':\n",
        "    new_labels[i]=0\n",
        "  elif new_labels[i]=='Polyps':\n",
        "    new_labels[i]=1\n",
        "    \n",
        "# print(\"new_labels:\",new_labels)\n",
        "\n",
        "new_labels = np.eye(len(np.unique(new_labels)))[new_labels.astype(int)]"
      ],
      "metadata": {
        "id": "lP9DSU1sn7Vq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7ea912-19f5-4de5-c955-34b9eb66d4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels: ['Polyps', 'Polyps', 'Polyps', 'Polyps', 'Polyps', 'Polyps', 'Polyps', 'Polyps', 'Polyps', 'Polyps', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy']\n",
            "new_labels: ['1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_labels)\n",
        "# final_labels = []\n",
        "for i in range(len(new_labels)):\n",
        "  final_labels.append(int(new_labels[i][1]))\n",
        "# print(final_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wPxjf4Y4jyH",
        "outputId": "dec1b79b-1a83-421b-9767-af35e4320d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features, final_labels, test_size=0.2, random_state=2)"
      ],
      "metadata": {
        "id": "ZKHoq1DxoE4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the MLP classifier model on the training set\n",
        "\n",
        "clf = MLPClassifier(activation='relu', alpha=0.0001, solver='adam', hidden_layer_sizes=(64,), max_iter=1000)\n",
        "clf.fit(X_train.reshape((X_train.shape[0], -1)), Y_train)\n"
      ],
      "metadata": {
        "id": "IabcA5N9oF7L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "1367a836-3912-46db-83e3-51a94a670fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(64,), max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(64,), max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(64,), max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "score = clf.score(X_train.reshape((X_train.shape[0], -1)), Y_train)\n",
        "print('Train accuracy:', score)\n",
        "score = clf.score(X_test.reshape((X_test.shape[0], -1)), Y_test)\n",
        "print('Test accuracy:', score)"
      ],
      "metadata": {
        "id": "s8eMzNUHoMXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2712dd4-50c4-497f-fd0a-d5dc4bb324ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.6818181818181818\n",
            "Test accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hyperparameter Tuning of the MLP classifier"
      ],
      "metadata": {
        "id": "eSxMr9L52TIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a grid of hyperparameters to search over\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(128,), (256,), (512,)],\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['adam', 'sgd', 'lbfgs'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "    'max_iter': [500, 1000, 2000]\n",
        "}\n",
        "\n",
        "# perform a grid search to find the best hyperparameters\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "clf = MLPClassifier()\n",
        "grid = GridSearchCV(clf, param_grid, cv=3, verbose=2, n_jobs=-1)\n",
        "grid.fit(X_train.reshape((X_train.shape[0], -1)), Y_train)\n",
        "\n",
        "# print the best hyperparameters and validation score\n",
        "print('Best hyperparameters:', grid.best_params_)\n",
        "print('Validation accuracy:', grid.best_score_)"
      ],
      "metadata": {
        "id": "Y0_Nm2c9Xfzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be60f477-c31b-429a-94bd-368bc541db43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 729 candidates, totalling 2187 fits\n",
            "Best hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (128,), 'learning_rate': 'constant', 'max_iter': 500, 'solver': 'adam'}\n",
            "Validation accuracy: 0.6845238095238096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the MLP classifier model on the training set with the best hyperparameters\n",
        "clf2 = MLPClassifier(activation= 'relu',\n",
        "                    alpha= 0.0001,\n",
        "                    hidden_layer_sizes= (128,),\n",
        "                    learning_rate= 'constant',\n",
        "                    max_iter= 500,\n",
        "                    solver= 'adam')\n",
        "clf2.fit(X_train.reshape((X_train.shape[0], -1)), Y_train)"
      ],
      "metadata": {
        "id": "7BtIChSTyBYs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "85c69dfa-f0e5-4dae-b7ca-ab675d04e683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(128,), max_iter=500)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(128,), max_iter=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(128,), max_iter=500)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "score = clf2.score(X_train.reshape((X_train.shape[0], -1)), Y_train)\n",
        "print('Train accuracy:', score)\n",
        "score = clf2.score(X_test.reshape((X_test.shape[0], -1)), Y_test)\n",
        "print('Test accuracy:', score)"
      ],
      "metadata": {
        "id": "y9IqTmS6yzUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f8def2-f9fa-41ef-cdcb-746a0a37db7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.6818181818181818\n",
            "Test accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aI0A7JwyzMBr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}